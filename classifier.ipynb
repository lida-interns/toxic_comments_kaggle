{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition: Classifiers approach\n",
    "    \n",
    "### TO DO:\n",
    "\n",
    "1. Corpus (train.csv) partition into training & validation sets\n",
    "2. Taking into account imbalance labels\n",
    "3. Import required libraries.\n",
    "4. Create doc2vec for training & validation sets\n",
    "5. Create classifier model on training set\n",
    "6. Test model on validation set\n",
    "7. Do CV using 5-folds or 10-folds ( Repeatting 5 & 6 ) to choose the best group of hyper-parameters\n",
    "8. Crerate final model using the whole dataset with the best group of hyper-parameters\n",
    "9. Create submition model using test.csv\n",
    "\n",
    "###Note: Tree-Based Algorithms (Decision tree, Random Forests, Gradient Boosted Trees, etc.) perform well on imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble.GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import map_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import (EditedNearestNeighbours,\n",
    "                                     RepeatedEditedNearestNeighbours,\n",
    "                                     TomekLinks,\n",
    "                                     ClusterCentroids)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import (SMOTETomek,SMOTEENN)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open dataset\n",
    "path = \"M:\\\\__Kaggle\\\\toxic_comments_kaggle\" #your working path\n",
    "os.chdir(path) #change working directory\n",
    "df = pd.read_csv(\"train.csv\") #open dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.columns.values[2:]].astype(str).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hotv'] = df[df.columns.values[2:]].astype(str).apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'making wikipedia a better and more inviting place.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-9714f45bae03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Apply SMOTE + ENN (over- and under-sampling algorithms)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdata_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Make the splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_sample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\imblearn\\combine\\smote_enn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \"\"\"\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratio_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'making wikipedia a better and more inviting place.'"
     ]
    }
   ],
   "source": [
    "#Dataset partition\n",
    "#https://github.com/scikit-learn-contrib/imbalanced-learn\n",
    "#http://contrib.scikit-learn.org/imbalanced-learn/stable/api.html\n",
    "data = df.comment_text.astype(str).as_matrix()\n",
    "labels = df.hotv.as_matrix()\n",
    "\n",
    "data = df.comment_text.values.reshape(-1, 1)\n",
    "labels = df.hotv.as_matrix()\n",
    "\n",
    "#X_arr = data.values\n",
    "#y_arr = labels.values\n",
    "\n",
    "#Sampling to improve labels imbalance\n",
    "# Apply SMOTE + ENN (over- and under-sampling algorithms)\n",
    "sme = SMOTEENN()\n",
    "data_resampled, labels_resampled = sme.fit_sample(data, labels)\n",
    "\n",
    "# Make the splits\n",
    "X_train, X_val, labels_train, labels_val = train_test_split(data_resampled, labels_resampled, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conver documents to vector representation (doc2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['100000', '000000', '000000', ..., '000000', '000000', '000000'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forests\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=50)\n",
    "clf_rf.fit(train_x, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['100000', '000000', '000000', ..., '000000', '000000', '000000'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([['aaa'] * 100, ['bbb'] * 100]).T\n",
    "y = np.array([0] * 10 + [1] * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb'],\n",
       "       ['aaa', 'bbb']],\n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
